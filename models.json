{
    "models": [
        {
            "name": "llama2",
            "type": "llama",
            "parameters": "7B",
            "description": "Meta's Llama 2 model",
            "arch": "llama2",
            "quantization": "Q4_K_M",
            "memory": "5GB",
            "runnable": "llama2:latest"
        },
        {
            "name": "mistral",
            "type": "AI",
            "parameters": "7B",
            "description": "Mistral AI's model",
            "arch": "mistral",
            "quantization": "Q4_K_M",
            "memory": "4.8GB",
            "runnable": "mistral:latest"
        },
        {
            "name": "dolphin-phi",
            "type": "AI",
            "parameters": "2.7B",
            "description": "Phi-2 based model",
            "arch": "phi",
            "quantization": "Q4_K_M",
            "memory": "2.5GB",
            "runnable": "dolphin-phi:latest"
        },
        {
            "name": "neural-chat",
            "type": "AI",
            "parameters": "7B",
            "description": "Intel's neural chat model",
            "arch": "neural",
            "quantization": "Q4_K_M",
            "memory": "4.8GB",
            "runnable": "neural-chat:latest"
        },
        {
            "name": "deepseek-coder",
            "type": "AI",
            "parameters": "6.7B",
            "description": "Coding specialized model",
            "arch": "deepseek",
            "quantization": "Q4_K_M",
            "memory": "4.5GB",
            "runnable": "deepseek-coder:latest"
        },
        {
            "name": "codellama",
            "type": "llama",
            "parameters": "7B",
            "description": "Meta's Code Llama model",
            "arch": "codellama",
            "quantization": "Q4_K_M",
            "memory": "4.8GB",
            "runnable": "codellama:latest"
        },
        {
            "name": "phi",
            "type": "AI",
            "parameters": "2.7B",
            "description": "Microsoft's Phi-2 model",
            "arch": "phi",
            "quantization": "Q4_K_M",
            "memory": "2.5GB",
            "runnable": "phi:latest"
        },
        {
            "name": "qwen",
            "type": "AI",
            "parameters": "7B",
            "description": "Qwen model by Alibaba",
            "arch": "qwen",
            "quantization": "Q4_K_M",
            "memory": "5GB",
            "runnable": "qwen:latest"
        },
        {
            "name": "falcon",
            "type": "AI",
            "parameters": "7B",
            "description": "TII's Falcon model",
            "arch": "falcon",
            "quantization": "Q4_K_M",
            "memory": "4.8GB",
            "runnable": "falcon:latest"
        },
        {
            "name": "stable-beluga",
            "type": "AI",
            "parameters": "7B",
            "description": "Stable Beluga model",
            "arch": "llama",
            "quantization": "Q4_K_M",
            "memory": "4.8GB",
            "runnable": "stable-beluga:latest"
        },
        {
            "name": "gpt-4",
            "type": "AI",
            "parameters": "175B",
            "description": "OpenAI's GPT-4 model",
            "arch": "transformer",
            "quantization": "FP16",
            "memory": "700GB",
            "runnable": "gpt-4:latest"
        },
        {
            "name": "bloom",
            "type": "AI",
            "parameters": "176B",
            "description": "BigScience's BLOOM model",
            "arch": "transformer",
            "quantization": "FP16",
            "memory": "700GB",
            "runnable": "bloom:latest"
        },
        {
            "name": "opt",
            "type": "AI",
            "parameters": "175B",
            "description": "Meta's OPT model",
            "arch": "transformer",
            "quantization": "FP16",
            "memory": "700GB",
            "runnable": "opt:latest"
        },
        {
            "name": "chatglm",
            "type": "AI",
            "parameters": "6B",
            "description": "ChatGLM model for bilingual tasks",
            "arch": "glm",
            "quantization": "Q4_K_M",
            "memory": "4.5GB",
            "runnable": "chatglm:latest"
        },
        {
            "name": "alpaca",
            "type": "AI",
            "parameters": "7B",
            "description": "Stanford's Alpaca model",
            "arch": "llama",
            "quantization": "Q4_K_M",
            "memory": "4.8GB",
            "runnable": "alpaca:latest"
        }
    ]
}